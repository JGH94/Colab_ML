{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TensorBoard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnBv+sFLhQ5cgFj4UZWIua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JGH94/Colab_ML/blob/main/CNN_TensorBoard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrFp-82cw-xP"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTnc9mNjxcb6"
      },
      "source": [
        "(X_Train,Y_Train),(X_Test,Y_Test) = tf.keras.datasets.mnist.load_data()\n",
        "X_Train,X_Test = X_Train.astype('float32'), X_Test.astype('float32')\n",
        "X_Train, X_Test = X_Train.reshape([-1,784]), X_Test.reshape([-1, 784])\n",
        "X_Train, X_Test = X_Train/255., X_Test/255.\n",
        "Y_Train, Y_Test = tf.one_hot(Y_Train, depth= 10), tf.one_hot(Y_Test, depth= 10)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTDFNUpm1K9_"
      },
      "source": [
        "Train_Data= tf.data.Dataset.from_tensor_slices((X_Train,Y_Train))\n",
        "Train_Data= Train_Data.repeat().shuffle(60000).batch(50)\n",
        "Train_Data_Iter = iter(Train_Data)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrLv8bG_10Sr"
      },
      "source": [
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # Convolution layer \n",
        "    # 5 X 5 kernel Size / 32개 Filter\n",
        "    self.Conv_Layer_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation= 'relu')\n",
        "    self.Pool_Layer_1 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides =2)\n",
        "\n",
        "    # Convolustion Layer 2\n",
        "    # 5 X 5 kernel Size / 64개 Filter\n",
        "    self.Conv_Layer_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='same', activation= 'relu')\n",
        "    self.Pool_Layer_2 = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides =2)\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    # 7 X 7 크기 64개의 activation map 1024개로 변환\n",
        "    self.Flatten_Layer = tf.keras.layers.Flatten()\n",
        "    self.FC_Layer_1 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "\n",
        "    # Output Layer\n",
        "    # 1024개 특징을 10개의 클래스 one-hot-encoding으로 표현된 숫자로 변환\n",
        "    self.Output_Layer = tf.keras.layers.Dense(10, activation=None)\n",
        "  \n",
        "  def call(self, x):\n",
        "    #MNIST 데이터를 3차원 형태로 reshape\n",
        "    X_Image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "    # 28X28X1을 28X28X32로 변환, -> 14X14X32\n",
        "    Conv_1 = self.Conv_Layer_1(X_Image)\n",
        "    Pool_1 = self.Pool_Layer_1(Conv_1)\n",
        "    # 14X14X32를 14X14X64 변환, -> 7X7X64\n",
        "    Conv_2 = self.Conv_Layer_2(Pool_1)\n",
        "    Pool_2 = self.Pool_Layer_2(Conv_2)\n",
        "\n",
        "    #7X7X64 -> 1024\n",
        "    Pool_2_Flat = self.Flatten_Layer(Pool_2)\n",
        "    FC_1 = self.FC_Layer_1(Pool_2_Flat)\n",
        "\n",
        "    # 1024 -> 10\n",
        "    Logits = self.Output_Layer(FC_1)\n",
        "    Y_Pred = tf.nn.softmax(Logits)\n",
        "    return Y_Pred, Logits\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y745RRP08mOd"
      },
      "source": [
        "def Cross_Entropy_Loss(Logits, Y):\n",
        "  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Logits, labels=Y))\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFUfu0uC88LK"
      },
      "source": [
        "#최적화 Adam\n",
        "optimizer = tf.optimizers.Adam(1e-4)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnR8hbsa9G8L"
      },
      "source": [
        "# TensorBoard Summary 정보/ 저장할 폴더 경로 설정 및 FileWriter \n",
        "Train_Summary_Write = tf.summary.create_file_writer('./TensorBoard/Train')\n",
        "Test_Summary_Write = tf.summary.create_file_writer('./TensorBoard/Test')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c39EW1fG93K9"
      },
      "source": [
        "def train_step(model, x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred, logits = model(x)\n",
        "    loss = Cross_Entropy_Loss(logits, y)\n",
        "  # 매 step마다 tf.summary.scalar, tf.summary.image 텐서보드 로그를 기록합니다.\n",
        "  with Train_Summary_Write.as_default():\n",
        "    tf.summary.scalar('loss', loss, step=optimizer.iterations)\n",
        "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "    tf.summary.image('training image', x_image, max_outputs=10, step=optimizer.iterations) \n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk2f8aNT_kQr"
      },
      "source": [
        "def Compute_Accuracy(Y_Pred, Y, Summary_writer):\n",
        "  correct_pr = tf.equal(tf.argmax(Y_Pred,1), tf.argmax(Y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pr, tf.float32))\n",
        "\n",
        "  with Summary_writer.as_default():\n",
        "    tf.summary.scalar('accuracy',accuracy, step= optimizer.iterations)\n",
        "  return accuracy\n",
        "CNN_Model = CNN()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbQKqcI9nUrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfION7h5nNJg"
      },
      "source": [
        "save_path =\"./SaveModel\"\n",
        "Training_CK = tf.train.Checkpoint(step=tf.Variable(0), model = CNN_Model) \n",
        "Training_CK_Manager = tf.train.CheckpointManager(Training_CK, directory= save_path, max_to_keep= 5)\n",
        "Training_Last = tf.train.latest_checkpoint(save_path)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxEXtyFJn_E2",
        "outputId": "38f133c2-cadb-4571-96d6-ce83e9d05f0c"
      },
      "source": [
        "# 저장파일이 존재하면 불러와서 Restored하여 학습진행\n",
        "if Training_Last:\n",
        "  Training_CK.restore(Training_Last)\n",
        "  print(\"Epoch: %d, Train data 정확도: %f\" %(Training_CK.step, train_accuracy))  \n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Train data 정확도: 0.900000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkyv6K4knnsa",
        "outputId": "289d5c5b-7978-4668-b0fc-bf68ca778838"
      },
      "source": [
        "while int(Training_CK.step) <(1000 + 1):\n",
        "  batch_x, batch_y = next(Train_Data_Iter)\n",
        "  if Training_CK.step % 50 == 0:\n",
        "    Training_CK_Manager.save(checkpoint_number = Training_CK.step)\n",
        "    train_accuracy = Compute_Accuracy(CNN_Model(batch_x)[0],batch_y, Train_Summary_Write) \n",
        "    print(\"Epoch: %d, Train data 정확도: %f\" %(Training_CK.step, train_accuracy))  \n",
        "  \n",
        "  train_step(CNN_Model, batch_x,batch_y)\n",
        "  Training_CK.step.assign_add(1)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Train data 정확도: 0.160000\n",
            "Epoch: 150, Train data 정확도: 0.840000\n",
            "Epoch: 200, Train data 정확도: 0.780000\n",
            "Epoch: 250, Train data 정확도: 0.940000\n",
            "Epoch: 300, Train data 정확도: 0.920000\n",
            "Epoch: 350, Train data 정확도: 0.920000\n",
            "Epoch: 400, Train data 정확도: 0.920000\n",
            "Epoch: 450, Train data 정확도: 0.940000\n",
            "Epoch: 500, Train data 정확도: 0.960000\n",
            "Epoch: 550, Train data 정확도: 0.860000\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbcef8f4d90>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2820, in while_loop\n",
            "    return result  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2768, in <lambda>\n",
            "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/map_fn.py\", line 499, in compute\n",
            "    return (i + 1, tas)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/map_fn.py\", line 497, in <listcomp>\n",
            "    ta.write(i, value) for (ta, value) in zip(tas, result_value_batchable)  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "Epoch: 600, Train data 정확도: 0.900000\n",
            "Epoch: 650, Train data 정확도: 0.980000\n",
            "Epoch: 700, Train data 정확도: 0.900000\n",
            "Epoch: 750, Train data 정확도: 0.960000\n",
            "Epoch: 800, Train data 정확도: 0.940000\n",
            "Epoch: 850, Train data 정확도: 0.940000\n",
            "Epoch: 900, Train data 정확도: 1.000000\n",
            "Epoch: 950, Train data 정확도: 0.980000\n",
            "Epoch: 1000, Train data 정확도: 0.940000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Tp62_OqhME"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfDqtnrmBqPN",
        "outputId": "43eaf9e1-edfc-4c83-d0dc-9c927263e9f0"
      },
      "source": [
        "\n",
        "#for i in range(1000):\n",
        "  #batch_x, batch_y = next(Train_Data_Iter)\n",
        "  #if i % 50 == 0:\n",
        "    #train_accuracy = Compute_Accuracy(CNN_Model(batch_x)[0], batch_y, Train_Summary_Write)\n",
        "    #print(\"Epoch: %d, Train data 정확도: %f\" % (i, train_accuracy))\n",
        "  #train_step(CNN_Model, batch_x,batch_y) \n",
        "#print(\"정확도(Accuracy): %f\" % Compute_Accuracy(CNN_Model(X_Test)[0], Y_Test, Test_Summary_Write))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train data 정확도: 0.120000\n",
            "Epoch: 50, Train data 정확도: 0.680000\n",
            "Epoch: 100, Train data 정확도: 0.880000\n",
            "Epoch: 150, Train data 정확도: 0.880000\n",
            "Epoch: 200, Train data 정확도: 0.940000\n",
            "Epoch: 250, Train data 정확도: 0.960000\n",
            "Epoch: 300, Train data 정확도: 0.960000\n",
            "Epoch: 350, Train data 정확도: 0.900000\n",
            "Epoch: 400, Train data 정확도: 0.960000\n",
            "Epoch: 450, Train data 정확도: 1.000000\n",
            "Epoch: 500, Train data 정확도: 0.920000\n",
            "Epoch: 550, Train data 정확도: 0.960000\n",
            "Epoch: 600, Train data 정확도: 0.900000\n",
            "Epoch: 650, Train data 정확도: 0.980000\n",
            "Epoch: 700, Train data 정확도: 0.940000\n",
            "Epoch: 750, Train data 정확도: 0.980000\n",
            "Epoch: 800, Train data 정확도: 1.000000\n",
            "Epoch: 850, Train data 정확도: 0.920000\n",
            "Epoch: 900, Train data 정확도: 0.960000\n",
            "Epoch: 950, Train data 정확도: 0.980000\n",
            "정확도(Accuracy): 0.973400\n"
          ]
        }
      ]
    }
  ]
}